Sqoop Documentation:
--------------------
Sqoop is a Bi-directional tool to migrate data between RDBMS Systems to Hadoop/HBase/Hive vice versa.
It is having a list of commmands like 

Installation:
-------------
To install sqoop, do the following steps.

  1. Download Sqoop from the Apache Sqoop Website.
  2. Extract the tar file and place the extracted sqoop directory into your favourite directory.
  3. Modify the /home/$USER/.bashrc file to add HADOOP_HOME, SQOOP_HOME, and adding the executable of Hadoop, Sqoop to
     System Path variable called PATH, which makes commands like hadoop, sqoop are available acorss the System
  4. Download the JDBC Driver. Let us assume we are migrating data from MySQL to Hadoop, Download the MySQL JDBC driver
     from the MySQL website. This is true for other RDBMS Systems like Oracle, DB2 etc...
  5. Place the Downloaded JDBC driver into Hadoop lib directory as well as Sqoop lib directory.
  
Run the sqoop command in terminal, it will display try 'sqoop help'

cmd> sqoop help
  
usage: sqoop COMMAND [ARGS]

  Available commands:
    codegen            Generate code to interact with database records
    create-hive-table  Import a table definition into Hive
    eval               Evaluate a SQL statement and display the results
    export             Export an HDFS directory to a database table
    help               List available commands
    import             Import a table from a database to HDFS
    import-all-tables  Import tables from a database to HDFS
    job                Work with saved jobs
    list-databases     List available databases on a server
    list-tables        List available tables in a database
    merge              Merge results of incremental imports
    metastore          Run a standalone Sqoop metastore
    version            Display version information

  See 'sqoop help COMMAND' for information on a specific command
-----------------------------------------------------------------------------------------------------------
To Migrate a sample MySQL Table data from MySQL to Hadoop. Do the following things
MySQL is not running in your machine, please install it by refering Hadoop_Lab_Information.pdf document.

cmd> mysql -u username -p
password: "enter your password"

cmd> create database hadoop;
cmd> use hadoop;
cmd> create table stocks (exchange varchar(50), symbol varchar(50), stock varchar(50), volume bigint(11));

cmd> insert into stocks (exchange, symbol, stock, volume) values ('NYSE', 'GOOG', 'Google', 12000000);
cmd> insert into stocks (exchange, symbol, stock, volume) values ('BSE', 'SBI', 'State Bank of India', 100000);
cmd> insert into stocks (exchange, symbol, stock, volume) values ('BSE', 'INFY', 'Infosys Technologies Ltd', 1000000);
cmd> insert into stocks (exchange, symbol, stock, volume) values ('BSE', 'TCS', 'Tata Consultancy Services', 1500000);
-----------------------------------------------------------------------------------------------------------------

Sqoop Commands Uasage:

1. version
  
    sqoop version
    
    Sqoop 1.3.0
    git commit id 4859bdbf89fa3492db3c19c15054966e1bb1b628
    Compiled by arvind@ubuntu on Sat Jun 18 14:56:19 PDT 2011

2. codegen
  
    sqoop codegen --connect jdbc:mysql://localhost:3306/hadoop --table stocks --username root -P
    password: "eneter your db password for username"
    
    The above command will generate the class name called stocks.java in the current directory where the above command
    issued. The generated code is used to interact the with database records.
    
3. list-databases

    sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root -P
    
    The above command displays the all the databases in the MySQL server.
    
    Example Output:
    ---------------
    13/09/19 02:21:30 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
    13/09/19 02:21:31 INFO manager.SqlManager: Executing SQL statement: SHOW DATABASES
    information_schema
    hadoop
    hivemetastore
    mysql
    performance_schema
    test

4. list-tables 
    
    sqoop list-tables --connect jdbc:mysql://localhost:3306/hadoop --username root -P
    
    The above command will display all the tables under the database name called hadoop.
    
    Example Output:
    ---------------
    13/09/19 02:23:50 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
    indix
    nyse
    stocks

5. eval
    
    sqoop eval --connect jdbc:mysql://localhost:3306/hadoop --username root -P -e "select * from stocks limit 10"
    
    The above command is good for evaluating the SQL code, It is always better to check with eval before doing any bulk imports.

6. import
    
    sqoop import --connect jdbc:mysql://localhost:3306/hadoop --username root -P --table stocks -m 1 --target-dir /sqoop/stocks
    
    The above command will import the stocks table data into an HDFS directory called /sqoop/stocks
    
    sqoop-import -D mapreduce.job.reduces=2 --connect jdbc:mysql://10.17.80.90:3306/hadoop --username root --password root
    --table people --hbase-create-table --hbase-table people --column-family info --split-by sno --hbase-row-key sno -hbase-bulkload 
    -m 2 --target-dir /mydir/docbulkload
 
7. export

    sqoop export --connect jdbc:mysql://localhost:3306/hadoop --username root -P --export-dir /sqoop/stocks/ --table nyse

    The above command is used to export data from HDFS to MySQL table called nyse, Here MySQL field delimiter ','. If the
    data in HDFS file is delimited by ',' no need to any changes. If the data in HDFS is delimited by '\t' we have to add
    another flag like --fields-terminated-by '\t'.

8. import-all-tables

    sqoop import-all-tables --connect jdbc:mysql://localhost:3306/hadoop --username root -P -m 1  --warehouse-dir /hadoop/
    
    The above command will import all the tables of MySQL hadoop database into HDFS directory called /hadoop
    
9.   TO BE UPDATED........
      
        
